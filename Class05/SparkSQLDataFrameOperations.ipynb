{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Dataframe operations\n",
    "Spark DataFrames allow operations similar to pandas Dataframes. We demonstrate some of those.\n",
    "\n",
    "For more, see the [official guide](https://spark.apache.org/docs/latest/sql-programming-guide.html) and [this article](https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## But first, some important details\n",
    "Spark on datahub can run in two modes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* **local mode** which means that it is run on the same computer as the head node. This is convenient for small jobs and for debugging. Can also be done on your laptop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* **remote mode** in which the head node and the worker nodes are separate. This mode requires that the spark cluster is up and running. In this case the full resources of the clusters are available. This mode is useful when processing large jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T23:24:12.161432Z",
     "start_time": "2018-04-07T23:24:08.573341Z"
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType,BinaryType\n",
    "import os\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T23:24:12.161432Z",
     "start_time": "2018-04-07T23:24:08.573341Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.4 ms, sys: 10 ms, total: 31.4 ms\n",
      "Wall time: 5.08 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://e5f1b618f8b3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=test>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sc = SparkContext('local', 'test')\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T23:24:12.161432Z",
     "start_time": "2018-04-07T23:24:08.573341Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.67 ms, sys: 2.15 ms, total: 5.81 ms\n",
      "Wall time: 134 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x7fff9d2d6210>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import split,join,exists\n",
    "from os import mkdir,getcwd,remove\n",
    "from glob import glob\n",
    "\n",
    "# create directory if needed\n",
    "notebook_dir=getcwd()\n",
    "data_dir=join(split(notebook_dir)[0],'Data')\n",
    "weather_dir=join(data_dir,'Weather')\n",
    "\n",
    "file_index='NY'\n",
    "zip_file='%s.tgz'%(file_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "|    Station|Measurement|Year|              Values|       dist_coast|      latitude|         longitude|        elevation|state|             name|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "|USW00094704|   PRCP_s20|1945|[00 00 00 00 00 0...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_parquet = join(weather_dir,zip_file[:-3]+'parquet')\n",
    "print(weather_parquet)\n",
    "df = sqlContext.read.load(weather_parquet)\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T23:24:12.162491Z",
     "start_time": "2018-04-07T23:24:08.614Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Station: string (nullable = true)\n",
      " |-- Measurement: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- Values: binary (nullable = true)\n",
      " |-- dist_coast: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- elevation: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T23:24:12.163456Z",
     "start_time": "2018-04-07T23:24:08.616Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168398\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "|    Station|Measurement|Year|              Values|       dist_coast|      latitude|         longitude|        elevation|state|             name|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "|USW00094704|   PRCP_s20|1945|[00 00 00 00 00 0...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "CPU times: user 5.49 ms, sys: 858 Âµs, total: 6.34 ms\n",
      "Wall time: 1.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(df.count())\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### .describe()\n",
    "The method `df.describe()` computes five statistics for each column of the dataframe `df`.\n",
    "\n",
    "The statistics are: **count, mean, std, min,max**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "You get the following man page using the command `df.describe?`\n",
    "\n",
    "```\n",
    "Signature: df.describe(*cols)\n",
    "Docstring:\n",
    "Computes statistics for numeric and string columns.\n",
    "\n",
    "This include count, mean, stddev, min, and max. If no columns are\n",
    "given, this function computes statistics for all numerical or string columns.\n",
    "\n",
    ".. note:: This function is meant for exploratory data analysis, as we make no\n",
    "    guarantee about the backward compatibility of the schema of the resulting DataFrame.\n",
    "\n",
    ">>> df.describe(['age']).show()\n",
    "+-------+------------------+\n",
    "|summary|               age|\n",
    "+-------+------------------+\n",
    "|  count|                 2|\n",
    "|   mean|               3.5|\n",
    "| stddev|2.1213203435596424|\n",
    "|    min|                 2|\n",
    "|    max|                 5|\n",
    "+-------+------------------+\n",
    ">>> df.describe().show()\n",
    "+-------+------------------+-----+\n",
    "|summary|               age| name|\n",
    "+-------+------------------+-----+\n",
    "|  count|                 2|    2|\n",
    "|   mean|               3.5| null|\n",
    "| stddev|2.1213203435596424| null|\n",
    "|    min|                 2|Alice|\n",
    "|    max|                 5|  Bob|\n",
    "+-------+------------------+-----+\n",
    "\n",
    ".. versionadded:: 1.3.1\n",
    "File:      ~/spark-2.2.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\n",
    "Type:      method\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T23:24:12.164309Z",
     "start_time": "2018-04-07T23:24:08.693Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------------+\n",
      "|    Station|Measurement|              Year|\n",
      "+-----------+-----------+------------------+\n",
      "|     168398|     168398|            168398|\n",
      "|       NULL|       NULL|1963.4289124573927|\n",
      "|       NULL|       NULL|30.586766032145377|\n",
      "|USC00300015|       PRCP|              1871|\n",
      "|USW00094794|   TOBS_s20|              2013|\n",
      "+-----------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().select('Station','Measurement','Year').show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### groupby and agg\n",
    "The method `.groupby(col)` groups rows according the value of the column `col`.  \n",
    "The method `.agg(spec)` computes a summary for each group as specified in `spec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T23:24:12.165180Z",
     "start_time": "2018-04-07T23:24:08.732Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+---------+\n",
      "|Measurement|count(Station)|min(Year)|\n",
      "+-----------+--------------+---------+\n",
      "|   TMIN_s20|         13442|     1873|\n",
      "|       TMIN|         13442|     1873|\n",
      "|   SNOW_s20|         15629|     1884|\n",
      "|       TOBS|         10956|     1876|\n",
      "|   SNWD_s20|         14617|     1888|\n",
      "|   PRCP_s20|         16118|     1871|\n",
      "|   TOBS_s20|         10956|     1876|\n",
      "|       TMAX|         13437|     1873|\n",
      "|       SNOW|         15629|     1884|\n",
      "|   TMAX_s20|         13437|     1873|\n",
      "|       SNWD|         14617|     1888|\n",
      "|       PRCP|         16118|     1871|\n",
      "+-----------+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('Measurement').agg({'Year': 'min', 'Station':  'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T23:24:12.165969Z",
     "start_time": "2018-04-07T23:24:08.741Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# THis command will load the python module that defines the SQL functions\n",
    "#%load ls ~/spark-latest/python/pyspark/sql/functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using SQL queries on DataFrames\n",
    "\n",
    "There are two main ways to manipulate  DataFrames:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Imperative manipulation\n",
    "Using python methods such as `.select` and `.groupby`.\n",
    "* Advantage: order of operations is specified.\n",
    "* Disrdavantage : You need to describe both **what** is the result you want and **how** to get it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Declarative Manipulation (SQL)\n",
    "* Advantage: You need to describe only **what** is the result you want.\n",
    "* Disadvantage: SQL does not have primitives for common analysis operations such as **covariance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Using sql commands on a dataframe\n",
    "Spark supports a [subset](https://spark.apache.org/docs/latest/sql-programming-guide.html#supported-hive-features) of the Hive SQL query language.\n",
    "\n",
    "For example, You can use [Hive `select` syntax](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select) to select a subset of the rows in a dataframe.\n",
    "\n",
    "To use sql on a dataframe you need to first `register` it as a `TempTable`.\n",
    "\n",
    "for variety, we are using here a small dataframe loaded from a JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Counting the number of occurances of each measurement, imparatively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T23:24:12.169470Z",
     "start_time": "2018-04-07T23:24:08.919Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.34 ms, sys: 2.28 ms, total: 7.62 ms\n",
      "Wall time: 1.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "L=df.groupBy('measurement').count().collect()\n",
    "#L is a list of Rows (collected DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T23:24:12.170317Z",
     "start_time": "2018-04-07T23:24:08.921Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common mesurements\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('PRCP_s20', 16118),\n",
       " ('PRCP', 16118),\n",
       " ('SNOW_s20', 15629),\n",
       " ('SNOW', 15629),\n",
       " ('SNWD_s20', 14617),\n",
       " ('SNWD', 14617)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D=[(e.measurement,e['count']) for e in L]\n",
    "print('The most common mesurements')\n",
    "sorted(D,key=lambda x:x[1], reverse=True)[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most rare mesurements\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('TOBS', 10956),\n",
       " ('TOBS_s20', 10956),\n",
       " ('TMAX', 13437),\n",
       " ('TMAX_s20', 13437),\n",
       " ('TMIN_s20', 13442),\n",
       " ('TMIN', 13442)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The most rare mesurements')\n",
    "sorted(D,key=lambda x:x[1], reverse=False)[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Counting the number of occurances of each measurement, declaratively.\n",
    "\n",
    "#### Registrering a dataframe as a table in a database\n",
    "\n",
    "In order to apply SQL commands to a dataframe, it has to first be registered as a table in the database managed by sqlContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T23:24:12.171110Z",
     "start_time": "2018-04-07T23:24:08.956Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sqlContext.registerDataFrameAsTable(df,'weather') #using older sqlContext instead of newer (V2.0) sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T23:24:12.171758Z",
     "start_time": "2018-04-07T23:24:08.958Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SELECT measurement,COUNT(measurement) AS count,\n",
      "                   MIN(year) AS MinYear \n",
      "FROM weather  \n",
      "GROUP BY measurement \n",
      "ORDER BY count DESC\n",
      "\n",
      "+-----------+-----+-------+\n",
      "|measurement|count|MinYear|\n",
      "+-----------+-----+-------+\n",
      "|   PRCP_s20|16118|   1871|\n",
      "|       PRCP|16118|   1871|\n",
      "|   SNOW_s20|15629|   1884|\n",
      "|       SNOW|15629|   1884|\n",
      "|   SNWD_s20|14617|   1888|\n",
      "+-----------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 2.53 ms, sys: 3.47 ms, total: 6 ms\n",
      "Wall time: 1.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query=\"\"\"\n",
    "SELECT measurement,COUNT(measurement) AS count,\n",
    "                   MIN(year) AS MinYear \n",
    "FROM weather  \n",
    "GROUP BY measurement \n",
    "ORDER BY count DESC\n",
    "\"\"\"\n",
    "print(query)\n",
    "sqlContext.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Performing a map command\n",
    "* Dataframes do not support `map` and `reduce` operations.\n",
    "* In order to perform a `map` or `reduce` on a dataframe, you first need to transform it into an RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* This is a quick-and-dirty solution. \n",
    "* A better way is to use [built-in sparkSQL functions](https://spark.apache.org/docs/latest/api/sql/index.html)\n",
    "* Or if you can't find what you need, you can try and create a [User-Defined-function* (UDF)](https://spark.apache.org/docs/latest/sql-ref-functions-udf-scalar.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T23:24:12.172671Z",
     "start_time": "2018-04-07T23:24:09.025Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('USW00094704', 1945),\n",
       " ('USW00094704', 1946),\n",
       " ('USW00094704', 1947),\n",
       " ('USW00094704', 1948),\n",
       " ('USW00094704', 1949)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.map(lambda row:(row.Station,row.Year)).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Aggregations \n",
    "* **Aggregation** can be used, in combination with built-in sparkSQL functions \n",
    "to compute statistics of a dataframe.\n",
    "* computation will be fast thanks to combined optimzations with database operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A partial list : `count(), approx_count_distinct(), avg(), max(), min()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Of these, the interesting one is `approx_count_distinct()` which uses sampling to get an approximate count fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T23:24:12.173788Z",
     "start_time": "2018-04-07T23:24:09.163Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F # used here just for show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T23:24:12.174938Z",
     "start_time": "2018-04-07T23:24:09.168Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|approx_count_distinct(station)|\n",
      "+------------------------------+\n",
      "|                           339|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({'station':'approx_count_distinct'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Approximate Quantile\n",
    "\n",
    "* Suppose we want to partition the years into 10 ranges\n",
    "* such that in each range we have approximately the same number of records.\n",
    "* The method `.approxQuantile` will use a sample to do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T23:24:12.176464Z",
     "start_time": "2018-04-07T23:24:09.207Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with accuracy 0.1:  [1871.0, 1926.0, 1947.0, 1957.0, 1966.0, 1969.0, 1980.0, 1989.0, 2013.0]\n",
      "CPU times: user 12.2 ms, sys: 3.98 ms, total: 16.2 ms\n",
      "Wall time: 1.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('with accuracy 0.1: ',df.approxQuantile('Year', [0.1*i for i in range(1,10)], 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T23:24:12.176464Z",
     "start_time": "2018-04-07T23:24:09.207Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with accuracy 0.001:  [1917.0, 1937.0, 1949.0, 1957.0, 1966.0, 1975.0, 1984.0, 1993.0, 2003.0]\n",
      "CPU times: user 11.6 ms, sys: 3.24 ms, total: 14.8 ms\n",
      "Wall time: 1.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('with accuracy 0.001: ',df.approxQuantile('Year', [0.1*i for i in range(1,10)], 0.00001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reading rows selectively from Parquet\n",
    "Suppose we are only interested in snow measurements. We can apply an SQL query directly to the \n",
    "parquet files. As the data is organized in columnar structure, we can do the selection efficiently without loading the whole file to memory.\n",
    "\n",
    "Here the file is small, but in real applications it can consist of hundreds of millions of records. In such cases loading the data first to memory and then filtering it is very wasteful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T23:24:12.180490Z",
     "start_time": "2018-04-07T23:24:09.286Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT station,measurement,year \n",
      "FROM parquet.`NY.parquet` \n",
      "WHERE measurement=\"SNOW\" \n",
      "15629 ['station', 'measurement', 'year']\n",
      "+-----------+-----------+----+\n",
      "|    station|measurement|year|\n",
      "+-----------+-----------+----+\n",
      "|USC00308600|       SNOW|1932|\n",
      "|USC00308600|       SNOW|1956|\n",
      "|USC00308600|       SNOW|1957|\n",
      "|USC00308600|       SNOW|1958|\n",
      "|USC00308600|       SNOW|1959|\n",
      "+-----------+-----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query=\"\"\"SELECT station,measurement,year \n",
    "FROM parquet.`%s` \n",
    "WHERE measurement=\\\"SNOW\\\" \"\"\"%weather_parquet\n",
    "print(query)\n",
    "df2 = sqlContext.sql(query)\n",
    "print(df2.count(),df2.columns)\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "* Dataframes can be manipulated decleratively, which allows for more optimization.\n",
    "* Dataframes can be stored and retrieved from Parquet files.\n",
    "* It is possible to refer directly to a parquet file in an SQL query.\n",
    "* See you next time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## References\n",
    "* For an introduction to Spark SQL and Dataframes see: [Spark SQL, DataFrames](https://spark.apache.org/docs/latest/sql-programming-guide.html#spark-sql-dataframes-and-datasets-guide)\n",
    "* Also [spark-dataframe-and-operations](https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/) from [analyticsvidhya.com](https://www.analyticsvidhya.com)\n",
    "\n",
    "For complete API reference see\n",
    "* [SQL programming guide](https://spark.apache.org/docs/latest/sql-programming-guide.html) For Java, Scala and Python (Implementation is first in Scala and Python, later pyspark)\n",
    "* [pyspark API for the DataFrame class](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame) \n",
    "* [pyspark API for the pyspark.sql module](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark-sql-module)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "263px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
